\documentclass[paper=A4, pagesize, fontsize=6.95pt, DIV=calc]{scrartcl}
\usepackage[margin=.25cm]{geometry}
\usepackage{parskip}
\usepackage[singlespacing]{setspace}

\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{amsmath, amsfonts}

\RedeclareSectionCommand[
  beforeskip=0pt,
  afterindent=false
]{paragraph}

\begin{document}

If infinite collection of disjoint events, then $P(A_1 \cup A_2 \cup A_3 \cup \ldots) = \sum_{i=1}^{\infty}P(A_i)$ \\
$P(A \cup B) = P(A) + P(B) = P(A \cap B)$ \quad $P(A \cap B^\prime) = P(A \cup B) - P(B)$ \\
$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$ \\
For $N$ equally likely outcomes, $p = \frac{1}{N}$. For event $A$ with $N(A)$ being the number of outcomes contained in $A$, $P(A) = \frac{N(A)}{N}$

\paragraph{Product Rule}
Suppose a set consists of ordered collections of $k$ elements ($k$-tuples) and that there are $n_1$ possible choices for the first element; for each choice of the first element, there are $n_2$ possible choices of the second element;â€¦; for each possible choice of the first $k - 1$ elements, there are $n_k$ choices of the $k$th element. Then there are $n_1n_2 \cdot \cdots \cdot n_k$ possible $k$-tuples. \\

$P_{k,n} = \frac{n!}{n - k}!$ \quad $\binom{n}{k} = \frac{P_{k,n}}{k!}$ \quad $P(A|B) = \frac{P(A \cap B)}{P(B)}$ \quad $P(A \cap B) = P(A | B) \cdot P(B)$

\paragraph{Law of total Probability}
Let $A_1, \ldots, A_k$ be mutually exclusive and exhaustive events. Then for any other event $B$, $P(B) = \sum_{i=1}^{k}P(B|A_i)P(A_i)$. Events are mutually exclusive if no tw have any common outcomes. They are exhaustive if one $A_i$ must occur, so that $A_i \cup \ldots \cup A_k = \mathcal{S}$.

\paragraph{Bayes' Theorem}
Given prior probabilities $P(A_i)$ and conditional probabilities $P(B|A_i)$, the posterior probability is denoted as $P(A_j|B)$. Let $A_1, A_2, \ldots, A_k$ be a collection of k mutually exclusive and exhaustive events with \textit{prior} probabilities $P(A_i) (i = 1, \ldots, k)$. Then for any other event $B$ for which $P(B) > 0$, the posterior probability of $A_j$ given that $B$ has occurred is $P(A_j|B) = \frac{P(A_j \cup B)}{P(B)} = \frac{P(B|A_j)P(A_j)}{\sum_{i=1}^{k}P(B|A_i) \cdot P(A_i)}$ for $j = 1, \ldots, k$.

\paragraph{Independence}
Two events $A$ and $B$ are independent if $P(A|B) = P(A)$ and are dependent otherwise. This also implies $A^\prime$ and $B$, $A^\prime$ and $B^\prime$, and $A$ and $B^\prime$ are independent. $A$ and $B$ are independent if and only if (iff) $P(A \cap B) = P(A) \cdot P(B)$.

\paragraph{Mutual Independence}
Events $A_1, \ldots, A_n$ are mutually independent if for every $k(k = 2, 3, \ldots, n)$ and every subset of indices $i_1, i_2, \ldots, i_k$, $P(A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) = P(A_{i_1}) \cdot P(A_{i_2}) \cdot \cdots \cdot P(A_{i_k})$. In other words, the events are mutually independent if the probability of the intersection of any subset of the n events is equal to the product of the individual probabilities.

\paragraph{Random Variable}
For a given sample space $\mathcal{S}$ of some experiment, a random variable (rv) is any rule that associates a number with each outcome in $\mathcal{S}$. In mathematical language, a random variable is a function whose domain is the sample space and whose range is the set of real numbers.

Any random variable whose only possible values are 0 and 1 is called a \textbf{Bernoulli random variable}. A \textbf{discrete} random variable is an rv whose possible values either constitute a finite set or else can be listed in an infinite sequence in which there is a first element, a second element, and so on (``countably'' infinite). A random variable is \textbf{continuous} if both of the following apply: (1) Its set of possible values consists either of all numbers in a single interval on the number line (possibly infinite in extent, e.g., from $-\infty$ to $\infty$) or all numbers in a disjoint union of such intervals (e.g., $[0, 10] \cup [20, 30]$) (2) No possible value of the variable has positive probability, that is, $P(X = c) = 0$ for any possible value $c$.

The probability distribution or \textbf{probability mass function} (pmf) of a discrete rv is defined for every number $x$ by $p(x) = P(X = x) = P(\text{all}\ \omega \in \mathcal{S}: X(\omega) = x)$.

Suppose $p(x)$ depends on a quantity that can be assigned any one of a number of possible values, with each different value determining a different probability distribution. Such a quantity is called a \textbf{parameter} of the distribution. The collection of all probability distributions for different values of the parameter is called a \textbf{family} of probability distributions.

The \textbf{cumulative distribution function} (cdf) $F(x)$ of a discrete rv variable X with pmf $p(x)$ is defined for every number $x$ by $F(X) = P(X \le x) = \sum_{y: y\le x}p(y)$. For any number $x$, $F(x)$ is the probability that the observed value of $X$ will be at most $x$.

For any two numbers $a$ and $b$ with $a \le b$, $P(a \le X \le b) = F(b) - F(a-)$  where ``$a-$'' represents the largest possible $X$ value that is strictly less than $a$. In particular, if the only possible values are integers and if $a$ and $b$ are integers, then $P(a \le X \le b) = P(X = a\ \text{or}\ $a$ + 1\ \text{or}\ldots\ \text{or}\ b) = F(b) - F(a - 1)$. Taking $a = b$ yields $P(X = a) = F(a) - F(a - 1)$ in this case.

Let $X$ be a discrete rv with set of possible values $D$ and pmf $p(x)$. The \textbf{expected value} or \textbf{mean value} of $X$, denoted by $E(X)$ or $\mu_{X}$ or just $\mu$, is $E(X) = \mu_{X} = \sum_{x \in D}x \cdot p(x)$.

If the rv $X$ has a set of possible values $D$ and pmf $p(x)$, then the expected value of any function $h(X)$, denoted by $E[h(X)]$ or $\mu_{h(X)}$, is computed by $E[h(X)] = \sum_{D}h(x) \cdot p(x)$.

Expected value of a linear function: $E(aX + b) = a \cdot E(X) + b$

Let $X$ have pmf $p(x)$ and expected value $m$. Then the variance of $X$, denoted by $V(X)$ or $\sigma_X^2$, or just $\sigma^2$, is $V(X) = \sum_{D}(x - \mu)^2 \cdot p(x) = E[(X - \mu)^2]$. A shortcut is $V(X) = E(X^2) - [E(X)]^2$. The \textbf{standard deviation} (SD) of $X$ is $\sigma_X = \sqrt{\sigma_X^2}$.

$V(aX + b) = a^2 \cdot \sigma_X^2$ and $\sigma_{aX + b} = |a| \cdot \sigma_X$. In particular, $\sigma_{aX} = |a| \cdot \sigma_X, \sigma_{X + b} = \sigma_X$.

\paragraph{Binomial Experiment}
(1) The experiment consists of a sequence of $n$ smaller experiments called trials, where $n$ is fixed in advance of the experiment. (2) Each trial can result in one of the same two possible outcomes (dichotomous trials), which we generically denote by success ($S$) and failure ($F$). (3) The trials are independent, so that the outcome on any particular trial does not influence the outcome on any other trial. (4) The probability of success $P(S)$ is constant from trial to trial; we denote this probability by $p$.

Consider sampling without replacement from a dichotomous population of size $N$. If the sample size (number of trials) $n$ is at most 5\% of the population size, the experiment can be analysed as though it were a binomial experiment.

The \textbf{binomial random variable $X$} associated with a binomial experiment consisting of $n$ trials is defined as $X =$ the number of $S$'s among the $n$ trials. pmf $b(x; n, p) = \binom{n}{x}p^x(1 - p)^{n - x}$ for $x = 0, 1, 2, \ldots n$ (pmf = 0 otherwise).

$X \sim \text{Bin}(n, p)$ indicates that $X$ is a binomial rv based on $n$ trials with success probability $p$. For it, the cdf will be denoted by $B(x; n, p) = P(X \le x) = \sum_{y = 0}^{x}b(y; n, p)$ for $x = 0, 1, \ldots, n$. Furthermore, $E(X) = np$, $V(X) = np(1 - p) = npq$, and $\sigma_X = \sqrt{npq}$ (where $q = 1 - p$).

\paragraph{Hypergeometric Distribution}
(1) The population or set to be sampled consists of $N$ individuals, objects, or elements (a \textit{finite} population). (2) Each individual can be characterized as a success ($S$) or a failure ($F$), and there are $M$ successes in the population. (3) A sample of $n$ individuals is selected without replacement in such a way that each subset of size $n$ is equally likely to be chosen.

If X is the number of $S$'s in a completely random sample of size $n$ drawn from a population consisting of $M$ $S$'s and $(N - M)$ $F$'s, then the probability distribution of $X$, called the \textbf{hypergeometric distribution}, is given by $P(X = x) = h(x; n, M, N) = \frac{\binom{M}{x}\binom{N - M}{n - x}}{\binom{N}{n}}$ for $x$ and integer satisfying $\text{max}(0, n - N + M) \le x \le \text{min}(n, M)$.

$E(X) = np$ \quad $V(X) = \frac{N - n}{N - 1} \cdot np(1 - p)$ where $p = \frac{M}{N}$

\paragraph{Negative Binomial Distribution}
(1) The experiment consists of a sequence of independent trials. (2) Each trial can result in either a success ($S$) or a failure ($F$). The probability of success is constant from trial to trial, so $P(S\ \text{on trial}\ i) = p$ for $i = 1, 2, 3, \ldots$. (4) The experiment continues (trials are performed) until a total of $r$ successes have been observed, where $r$ is a specified positive integer. $X = $ the number of failures that precede the $r$th
success. In contrast to the binomial rv, the number of successes is fixed and the number of trials is random.

$nb(x; r, p) = \binom{x + r - 1}{r - 1}p^r(1 - p)^x$ for $x = 0, 1, 2, \ldots$.

In the special case $r = 1$, the pmf is $nb(x; 1, p) = (1 - p)^xp$ for $x = 0, 1, 2, \ldots$. Both $X =$ number of $F$'s and $Y =$ number of trials $(= 1 + X)$ are referred to in the literature as \textbf{geometric random variables}, and the previous pmf is called the \textbf{geometric distribution}.

\begin{tabular}{llllll}
  \textbf{$X$ is counting...}       & \textbf{pmf}              & \textbf{Formula}                   & \textbf{Alternative (equiv. binom)} & \textbf{Support}                           \\
  $k$ failures, given $r$ successes & $f(k; r, p) = P(X = k) =$ & $\binom {k+r-1}{k}p^{r}(1-p)^{k}$  & $\binom{k+r-1}{r-1}p^{r}(1-p)^{k}$  & for $k = 0, 1, 2, \ldots$                  \\
  $n$ trials, given $r$ successes   & $f(n; r, p) = P(x = n) =$ & $\binom{n-1}{r-1}p^{r}(1-p)^{n-r}$ & $\binom{n-1}{n-r}p^{r}(1-p)^{n-r}$  & for $n = r, r + 1, r + 2, \ldots$          \\
  $n$ trials, given $r$ failures    & $f(n; r, p) = P(x = n) =$ & $\binom{n-1}{r-1}p^{n-r}(1-p)^{r}$ & $\binom {n-1}{n-r}p^{n-r}(1-p)^{r}$ & for $n = r, r + 1, r + 2, \ldots$          \\
  $r$ successes, given $n$ trials   & $f(r; n, p) = P(x = r) =$ & \multicolumn{2}{l}{This is the binomial distribution: $\binom {n}{r}p^{r}(1-p)^{n-r}$} & for $r = 0, 1, 2, \ldots, n$ \\
\end{tabular}

$E(X) = \frac{r(1 - p)}{p}$ \quad $V(X) = \frac{r(1 - p)}{p^2}$

\paragraph{Poisson Distribution}
$p(x; \mu) = \frac{e^{-\mu}\cdot \mu^x}{x!}$ for $x = 0, 1, 2, 3, \ldots$ and $\mu > 0$. Suppose that in the binomial pmf $b(x; n, p)$, we let $n \rightarrow \infty$ and $p \rightarrow 0$ in such a way that $np$ approaches a value $\mu > 0$. Then $b(x; n, p) \rightarrow p(x; m)$. According to this result, in any binomial experiment in which $n$ is large and $p$ is small, $b(x; n, p) \approx p(x; m)$, where $\mu = np$. As a rule of thumb, this approximation can safely be applied if $n > 50$ and $np < 5$. $E(X) = V(X) = \mu$.

\end{document}
